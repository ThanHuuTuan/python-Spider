# 爬取我主良缘网用户信息并可视化
我本可以忍受黑暗<br>
如果我不曾见过太阳

### 技术点：
- [x] 动态加载网站：json数据提取
- [x] 如何将图片批量保存在本地

#### json数据提取
网站属于下拉动态加载网站，第一种也是最简单的方法就是通过分析AJAX来获取json数据。而我们也很迅速的就可以找到接口和数据，如何判断是否正确的获取到了数据：
```python
data = {
    'gender': '2',
    'marry':'1',
    'page':'1'
}
data['page']=i
url='http://www.lovewzly.com/api/user/pc/list/search?'
resp=requests.get(url,headers=headers,params=data)
if resp.status_code==200:
    data_json=resp.json()['data']['list']
```
一是基础太薄弱了，之前遇到国很多类似的网站，但是今日得到了反馈200却以为是上传数据的问题。200不就是对的反馈吗？之后就是json文件处理的问题，如何处理和提取json文件本来就是一件很简单的事情，但是不熟练导致了现在这样的情况时有发生。写一个部分总是需要借鉴之前的代码，这样的话确实就是个搬运工。<br>
关键词：<b>data_json=resp.json()<b>
 
 #### 如何批量保存图片到本地
 一开始以为是解析到了图片的url之后将图片保存在本地仍然是不可得，百度了许多文章，[为什么python3写的爬虫爬取到的图片无法打开？](https://segmentfault.com/q/1010000011782180)。参看了这篇博客之后才发现两个函数的url混淆在了一起刚好就发生了本文的问题。
```python
folder_path='./Photo'
if os.path.exists(folder_path)==False:
    os.makedirs(folder_path)
res=requests.get(url,headerh=headers)
# print(res)
# if url[0:5]=='http:':
#     img_url=url
# filename=url.split('/')[-1]
try:
    if 'png' in url:
        # print(url)
        fp = open('photo\\' +username+'.png', 'wb')
    else:
        fp = open('photo\\' + username + '.JPG', 'wb')
        # print(url)
    fp.write(res.content)
    print("Sucessful"+username)
    fp.close()
except:
    print("Failed"+username)
    pass
 ```
参考了许许多多的文章，有三个小的技术点：
- 创建文件夹，有了这个函数之后
