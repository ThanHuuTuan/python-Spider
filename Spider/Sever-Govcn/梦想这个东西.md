# 政府工作报告
梦想这个东西，是和诅咒一样的。不能实现自己梦想的人，即使中途放弃了，它也会阴魂不断，永远折磨着你。<br>
假面骑士555
## 技术点
- [x] 内容爬取和清洗
- [x] 分词和词频统计
- [x] 生成词云

### 内容爬取和清洗
目标网站：http://www.gov.cn/guowuyuan/2016-03/05/content_5049372.htm<br>
使用bs可以很迅速的获取到文本的内容：
```python
import requests
from bs4 import BeautifulSoup
def get_content(url):
    list=''
    content=requests.get(url)
    content.encoding=content.apparent_encoding
    html=BeautifulSoup(content.text,'lxml')
    div=html.find('div','article oneColumn pub_border')
    title=div.find('h1').text

    page=div.find('div','pages_content').text
    # print(title)
    ＃将内容写入到文本
    with open('2016国家工作报告.txt','a',encoding='utf-8')as f:
        f.write(title+'\n')
        f.write(page)
        f.close()
    print(page)
    return page
 
 政府工作报告
——2016年3月5日在第十二届全国人民代表大会第四次会议上
国务院总理 李克强

各位代表：
现在，我代表国务院，向大会报告政府工作，请予审议，并请全国政协各位委员提出意见。
一、2015年工作回顾
过去一年，我国发展面临多重困难和严峻挑战。在以习近平同志为总书记的党中央坚强领导下，全国各族人民以坚定的信心和非凡的勇气，攻坚克难，开拓进取，经济社会发展稳中有进、稳中有好，完成了全年主要目标任务，改革开放和社会主义现代化建设取得新的重大成就。
——经济运行保持在合理区间。国内生产总值达到67.7万亿元，增长6.9%，在世界主要经济体中位居前列。粮食产量实现"十二连增"，居民消费价格涨幅保持较低水平。特别是就业形势总体稳定，城镇新增就业1312万人，超过全年预期目标，成为经济运行的一大亮点。
——结构调整取得积极进展。服务业在国内生产总值中的比重上升到50.5%，首次占据"半壁江山"。消费对经济增长的贡献率达到66.4%。高技术产业和装备制造业增速快于一般工业。单位国内生产总值能耗下降5.6%。
```
对获取到的文本进行re的替换清洗，去除掉那些不需要的标签和数字等等<br>
```python
pattern = re.compile('[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？“”、~@#￥%……&*（）(\d+)]+')
result = pattern.sub("", get_content(url))
```
上面的方法是re的一种比较骚的写法，首先re.sub是需要三个参数的分别是（匹配对象，替代内容，匹配内容），上面不知为何却可以使用这样的骚操作就实现了替换。
### 分词与词频统计
结巴分词：
- [官方文档](https://pypi.org/project/jieba/)
- [python中文分词jieba十五分钟入门与进阶](https://link.juejin.im/?target=http%3A%2F%2Fblog.csdn.net%2Ffontthrone%2Farticle%2Fdetails%2F72782499)<br>
使用方法很简单：<br>
`words=[word for word in jieba.cut(text,cut_all=True) if len(word)>=2]`<br>
cut方法参数:(分词的字符串，cut_all是否打开全模式)，全模式的意思就是会全匹配。如：大傻逼，会分词成：大傻、傻逼和大傻逼。后面的加的判断是长度大于等于2才算一个词，得到的就是一个分词后的词的列表。<br>
接下来我们要统计词出现的频次和出现的频率。网上的套路大部分是通过：collection模块的Counter类来实现的，Counter是一个无序的容器类型，以字典的键值对形式存储，元素为key，计数为value，一般用来跟踪值出现的次数。
```python
c=Counter(words)
for word_freq in c.most_common(20):
    word,freq=word_freq
    print(word,freq)
```
上面的代码调用Counter(Words)就可以自动对上文分词进行一个统计，我们通过下面的for循环来输出前20的词的内容和次数，20可以随意更改。<br>
但是拿到了这些数据之后我们发现有一些关键词是没有意义的，比如推进、提高等动词，我们需要对数据进行过滤，最简单的就是定义一个过滤列表，把你想要过滤的词都丢进去，分词后遍历一波，把在过滤列表中存在的元素进行移出。
```python
exclude_words=[
        "中国","推进","全面","提高","工作","坚持","推动",
        "支持", "促进", "实施", "加快", "增加", "实现",
        "基本", "重大", "我国", "我们", "扩大", "继续",
        "以上", "取得", "地方", "今年", "加大", "优化",
    ]
for word in words:
    if word in exclude_words:
        words.remove(word)
结果：
发展 152
经济 90
改革 76
建设 71
社会 67
创新 61
政策 54
企业 48
服务 44
政府 42
人民 42
增长 42
加强 41
制度 37
结构 33
产业 29
安全 29
城镇 28
国家 28
国际 27
```
设置频率统计，jieba库里的analyse模块解决，次库支持停用词文件过滤，就是把不想要的分词都丢在一个文件里，分词时会自动过滤，调用的函数是extract_tag。analyse是需要导入的库。
```python
 exclude_words_file = '停用词.txt'
jieba.analyse.set_stop_words(exclude_words_file)
#获取关键词频率
tags=jieba.analyse.extract_tags(text,topK=100,withWeight=True)
for tag in tags:
    print(tag)
结果：
('发展', 0.07328328749611562)
('改革', 0.05019777471645185)
('建设', 0.0492624584141926)
('创新', 0.047409074738003244)
('加快', 0.03612380727373839)
('经济', 0.03531485730895738)
('加强', 0.032398535450197614)
('实施', 0.027378191342203213)
('政府', 0.02695337414552426)
('完善', 0.02372855274869341)
('政策', 0.02335916133983188)
('增长', 0.022528738991653148)
('社会', 0.022510502695676154)
('就业', 0.022485801984934373)
('企业', 0.021790595262240083)
('创业', 0.021631623399224302)
('扩大', 0.02046182314425011)
('制度', 0.019811235147597698)
('一批', 0.01959446603359829)
('深化', 0.018531550620115026)
('人民', 0.018437503968134494)
('落实', 0.01825697864415573)
('农村', 0.01795813072511429)
('试点', 0.017534705406706975)
('实现', 0.017242037592501105)
('安全', 0.017173168100740304)
('合作', 0.0162504886805191)
('我国', 0.016013269677882316)
('动能', 0.015978850409072407)
('机制', 0.01597559787553458)
('加大', 0.015551567962624982)
('服务业', 0.015431153033959593)
('城镇', 0.015194520852108834)
('我们', 0.015003618789219879)
('服务', 0.0149684207851836)
('取得', 0.0148717103402625)
('依法', 0.01465735505751364)
('积极', 0.01456824859605663)
('深入', 0.014155087437155286)
('结构性', 0.014103812930737354)
('民生', 0.013803171368764196)
('小康社会', 0.013488729140731456)
('工程', 0.01345675012476036)
('农业', 0.013407756139837782)
('脱贫', 0.013204692465300103)
('国际', 0.01319336912257779)
('供给', 0.012787419035748416)
('基本', 0.012744555090960034)
('改造', 0.012559918707518065)
('目标', 0.012513941512768028)
('群众', 0.012437184585459372)
('国家', 0.01238893278798997)
('地方', 0.012373094860974782)
('城乡', 0.01205762692590621)
('继续', 0.011957097759550216)
('重大', 0.011953748570284618)
('产能', 0.01195310645353635)
('保护', 0.011942938221987907)
('升级', 0.011927146577749594)
('培育', 0.011651910120402594)
('力度', 0.011636648066497566)
('今年', 0.011494644541743105)
('鼓励', 0.011447489872001181)
('抓好', 0.01143743773914762)
('地区', 0.011174728070452734)
('战略', 0.0110277268565831)
('国内', 0.01096608804166937)
('建成', 0.01094311431944551)
('投资', 0.010900065341986431)
('重点', 0.010844542564512609)
('坚决', 0.010814261199606254)
('生产总值', 0.010780676307004867)
('以上', 0.010724754480578086)
('提升', 0.010680783259772897)
('产业', 0.010476482629091578)
('领域', 0.010376027437152338)
('增强', 0.010258988421041145)
('持续', 0.010202436797174457)
('各位', 0.010144092401830115)
('改善', 0.010139466384491964)
('优化', 0.010110267711030822)
('教育', 0.010048770231116355)
('减少', 0.010028337384943225)
('问题', 0.00993654781630438)
('治理', 0.009915438557838076)
('稳定', 0.00991025465104262)
('各族人民', 0.009857543380032445)
('着力', 0.009828743380675417)
('必须', 0.009817320188942633)
('开展', 0.00980377844827754)
('住房', 0.009723455739876125)
('基础设施', 0.00967375454273116)
('特色', 0.00965768733655803)
('困难', 0.009492815669199232)
('解决', 0.00944966894005014)
('增加', 0.009441377824232414)
('消费', 0.009346674805509513)
('金融', 0.009291909776133315)
('城镇化', 0.009245707055938652)
('新型', 0.009115540557226073)
```
### 生成词云
- [十五分钟入门词云wordcloud](https://blog.csdn.net/fontthrone/article/details/72775865)
使用的还是原来的存储的文本，并没有使用进行一个筛选或者别的。而且在文本生成的这个过程中还是加入了结巴分词，所以我觉得如果要做数据过滤的话还是很简单的。其次是使用的之前使用的古柳写好的模板，我只是进行了一个调用然后进行了部分的修改。还是没有对云词进行一个深入的学习有机会再说把，虽然你有点low,但是我还是爱你的。
```python
def word_cloud(filename):
    text = open("{}.txt".format(filename)).read()
    # 结巴分词
    wordlist = jieba.cut(text, cut_all=True)
    wl = " ".join(wordlist)

    # 设置词云
    wc = WordCloud(
        # 设置背景颜色
        background_color="white",
        # 设置最大显示的词云数
        max_words=2000,
        # 设置一种电脑的字体
        font_path="C:\Windows\Fonts\simfang.ttf",
        height=1200,
        width=600,
        # 设置这种字体最大值
        max_font_size=100,
        # 设置有多少种随机生产状态，即多少配色方案
        random_state=30,
    )
    myword = wc.generate(wl)
    plt.imshow(myword)
    plt.axis("off")
    plt.show()
    wc.to_file("py_book.png")  # 保存下来

if __name__=='__main__':
    word_cloud('2016国家工作报告')
```
![图片](https://github.com/afrunk/Summer-for-Learing/blob/master/Spider/Sever-Govcn/py_book.png)
